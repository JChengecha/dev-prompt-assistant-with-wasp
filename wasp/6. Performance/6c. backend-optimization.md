# Backend Optimization Guide

## Overview
This guide organizes our existing backend optimization documentation into a structured workflow.

## Quick Reference
- [Optimization Basics](6a.%20optimization-basics.md)
- [Frontend Optimization](6b.%20frontend-optimization.md)
- [Database Optimization](6d.%20database-optimization.md)

## Server Configuration

### 1. Node.js Optimization
From our Node.js configuration:
```javascript
// server.js
const cluster = require('cluster')
const numCPUs = require('os').cpus().length

if (cluster.isMaster) {
  // Fork workers based on CPU cores
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork()
  }
  
  cluster.on('exit', (worker, code, signal) => {
    console.log(`Worker ${worker.process.pid} died`)
    // Replace the dead worker
    cluster.fork()
  })
} else {
  // Workers share the TCP connection
  const app = require('./app')
  app.listen(process.env.PORT)
}
```

### 2. Express Configuration
Reference our Express optimization:
```javascript
// app.js
const express = require('express')
const compression = require('compression')
const helmet = require('helmet')

const app = express()

// Enable gzip compression
app.use(compression({
  filter: (req, res) => {
    if (req.headers['x-no-compression']) {
      return false
    }
    return compression.filter(req, res)
  },
  level: 6 // Balanced setting between speed and compression
}))

// Security headers
app.use(helmet())

// Body parser with limits
app.use(express.json({ limit: '10kb' }))
app.use(express.urlencoded({ extended: true, limit: '10kb' }))
```

## Caching Strategy

### 1. Redis Caching
From our Redis implementation:
```javascript
// services/cache.js
const Redis = require('ioredis')
const { promisify } = require('util')

const redis = new Redis({
  host: process.env.REDIS_HOST,
  port: process.env.REDIS_PORT,
  password: process.env.REDIS_PASSWORD,
  maxRetriesPerRequest: 3
})

class CacheService {
  constructor(ttl = 3600) {
    this.ttl = ttl
  }
  
  async get(key) {
    const data = await redis.get(key)
    return data ? JSON.parse(data) : null
  }
  
  async set(key, value, ttl = this.ttl) {
    await redis.set(
      key,
      JSON.stringify(value),
      'EX',
      ttl
    )
  }
  
  async del(key) {
    await redis.del(key)
  }
  
  async flush() {
    await redis.flushall()
  }
}
```

### 2. Response Caching
```javascript
// middleware/cache.js
const cache = new CacheService()

function cacheMiddleware(duration) {
  return async (req, res, next) => {
    if (req.method !== 'GET') {
      return next()
    }
    
    const key = `__express__${req.originalUrl}`
    
    try {
      const cachedResponse = await cache.get(key)
      
      if (cachedResponse) {
        res.json(JSON.parse(cachedResponse))
        return
      }
      
      res.originalJson = res.json
      res.json = (body) => {
        cache.set(key, body, duration)
        res.originalJson(body)
      }
      
      next()
    } catch (error) {
      next(error)
    }
  }
}
```

## Request Processing

### 1. Request Queue
From our queue management:
```javascript
// services/queue.js
const Bull = require('bull')

class QueueService {
  constructor(name) {
    this.queue = new Bull(name, {
      redis: {
        host: process.env.REDIS_HOST,
        port: process.env.REDIS_PORT,
        password: process.env.REDIS_PASSWORD
      },
      defaultJobOptions: {
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 1000
        }
      }
    })
    
    this.queue.on('failed', (job, err) => {
      console.error(`Job ${job.id} failed:`, err)
    })
  }
  
  async add(data, options = {}) {
    return this.queue.add(data, options)
  }
  
  process(handler) {
    this.queue.process(async (job) => {
      try {
        return await handler(job.data)
      } catch (error) {
        console.error(`Error processing job ${job.id}:`, error)
        throw error
      }
    })
  }
}
```

### 2. Request Batching
```javascript
// services/batchProcessor.js
class BatchProcessor {
  constructor(options = {}) {
    this.batchSize = options.batchSize || 100
    this.flushInterval = options.flushInterval || 5000
    this.queue = []
    this.timeout = null
  }
  
  add(item) {
    this.queue.push(item)
    
    if (this.queue.length >= this.batchSize) {
      this.flush()
    } else if (!this.timeout) {
      this.timeout = setTimeout(() => this.flush(), this.flushInterval)
    }
  }
  
  async flush() {
    if (this.timeout) {
      clearTimeout(this.timeout)
      this.timeout = null
    }
    
    if (this.queue.length === 0) return
    
    const batch = this.queue.splice(0, this.batchSize)
    await this.processBatch(batch)
  }
  
  async processBatch(batch) {
    // Implementation specific to use case
  }
}
```

## Memory Management

### 1. Memory Monitoring
From our memory management:
```javascript
// monitoring/memory.js
const v8 = require('v8')

class MemoryMonitor {
  constructor(threshold = 0.9) {
    this.threshold = threshold
    this.interval = null
  }
  
  start(checkInterval = 60000) {
    this.interval = setInterval(() => {
      const stats = v8.getHeapStatistics()
      const usage = stats.used_heap_size / stats.heap_size_limit
      
      if (usage > this.threshold) {
        this.handleHighMemory(usage)
      }
    }, checkInterval)
  }
  
  handleHighMemory(usage) {
    console.warn(`High memory usage: ${(usage * 100).toFixed(2)}%`)
    
    if (global.gc) {
      global.gc()
    }
  }
  
  stop() {
    if (this.interval) {
      clearInterval(this.interval)
    }
  }
}
```

### 2. Stream Processing
```javascript
// services/streamProcessor.js
const { Transform } = require('stream')

class BatchTransform extends Transform {
  constructor(options = {}) {
    super({ objectMode: true })
    this.batchSize = options.batchSize || 1000
    this.batch = []
  }
  
  _transform(chunk, encoding, callback) {
    this.batch.push(chunk)
    
    if (this.batch.length >= this.batchSize) {
      this.push(this.batch)
      this.batch = []
    }
    
    callback()
  }
  
  _flush(callback) {
    if (this.batch.length > 0) {
      this.push(this.batch)
    }
    callback()
  }
}
```

## Performance Monitoring

### 1. Metrics Collection
From our metrics setup:
```javascript
// monitoring/metrics.js
const prometheus = require('prom-client')

// Enable default metrics
prometheus.collectDefaultMetrics()

// Custom metrics
const httpRequestDuration = new prometheus.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code']
})

const activeConnections = new prometheus.Gauge({
  name: 'active_connections',
  help: 'Number of active connections'
})

const jobQueueSize = new prometheus.Gauge({
  name: 'job_queue_size',
  help: 'Number of jobs in the queue'
})
```

### 2. Performance Tracing
```javascript
// monitoring/tracer.js
const opentelemetry = require('@opentelemetry/api')
const { NodeTracerProvider } = require('@opentelemetry/node')
const { SimpleSpanProcessor } = require('@opentelemetry/tracing')
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger')

const provider = new NodeTracerProvider()

const exporter = new JaegerExporter({
  endpoint: process.env.JAEGER_ENDPOINT,
  serviceName: 'wasp-api'
})

provider.addSpanProcessor(
  new SimpleSpanProcessor(exporter)
)

provider.register()

const tracer = opentelemetry.trace.getTracer('wasp-api')
```

## Best Practices

### Code Optimization
```javascript
// Best practices for code optimization
const optimizationGuidelines = {
  async: {
    // Use async/await consistently
    good: async function getData() {
      const result = await fetchData()
      return processResult(result)
    },
    // Avoid callback hell
    bad: function getData(callback) {
      fetchData((err, result) => {
        if (err) return callback(err)
        processResult(result, callback)
      })
    }
  },
  
  memory: {
    // Use streams for large data
    good: function processLargeFile() {
      return fs.createReadStream('large.file')
        .pipe(new Transform())
        .pipe(fs.createWriteStream('output.file'))
    },
    // Avoid loading entire file in memory
    bad: function processLargeFile() {
      const data = fs.readFileSync('large.file')
      return processData(data)
    }
  }
}
```

## Backend Checklist

### Server Configuration
- [ ] Node.js optimization
- [ ] Express settings
- [ ] Cluster setup
- [ ] Security headers
- [ ] Compression

### Caching
- [ ] Redis setup
- [ ] Response caching
- [ ] Query caching
- [ ] Static assets
- [ ] Cache invalidation

### Processing
- [ ] Request queuing
- [ ] Batch processing
- [ ] Stream handling
- [ ] Memory management
- [ ] Error handling

## Next Steps
1. Implement [Database Optimization](6d.%20database-optimization.md)
2. Set up [Monitoring](../7.%20Deployment/7d.%20monitoring-setup.md)
3. Configure [Security](../5.%20Security/5a.%20security-basics.md)

## Related Documentation
- [Performance Guide](6a.%20optimization-basics.md)
- [Database Guide](6d.%20database-optimization.md)
- [Monitoring Guide](../7.%20Deployment/7d.%20monitoring-setup.md)
