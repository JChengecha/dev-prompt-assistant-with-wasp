# Performance Testing Guide

## Overview
This guide organizes our existing performance testing documentation into a structured workflow.

## Quick Reference
- [Unit Testing](4a.%20unit-testing.md)
- [Integration Testing](4b.%20integration-testing.md)
- [E2E Testing](4c.%20e2e-testing.md)

## Load Testing

### 1. k6 Configuration
From our load testing setup:
```javascript
// tests/performance/config.js
export const options = {
  stages: [
    { duration: '1m', target: 50 },  // Ramp up
    { duration: '3m', target: 50 },  // Stay at 50 users
    { duration: '1m', target: 100 }, // Ramp up to 100
    { duration: '3m', target: 100 }, // Stay at 100
    { duration: '1m', target: 0 }    // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95% of requests should be below 500ms
    http_req_failed: ['rate<0.01']    // Less than 1% can fail
  }
}
```

### 2. Test Scenarios
Reference our load test scenarios:
```javascript
// tests/performance/scenarios/api-load.js
import http from 'k6/http'
import { check, sleep } from 'k6'
import { options } from '../config.js'

export default function() {
  const BASE_URL = 'http://localhost:3000/api'
  
  // Login
  const loginRes = http.post(`${BASE_URL}/auth/login`, {
    email: 'test@example.com',
    password: 'password123'
  })
  
  check(loginRes, {
    'logged in successfully': (r) => r.status === 200,
    'has token': (r) => r.json('token') !== undefined
  })
  
  const token = loginRes.json('token')
  
  // Get tasks
  const tasksRes = http.get(`${BASE_URL}/tasks`, {
    headers: {
      'Authorization': `Bearer ${token}`
    }
  })
  
  check(tasksRes, {
    'got tasks': (r) => r.status === 200,
    'has data': (r) => Array.isArray(r.json('data'))
  })
  
  sleep(1)
}
```

## Stress Testing

### 1. Artillery Configuration
From our stress testing setup:
```yaml
# artillery/config.yml
config:
  target: "http://localhost:3000"
  phases:
    - duration: 60
      arrivalRate: 5
      rampTo: 50
    - duration: 120
      arrivalRate: 50
    - duration: 60
      arrivalRate: 50
      rampTo: 100
  defaults:
    headers:
      Content-Type: "application/json"

scenarios:
  - name: "API stress test"
    flow:
      - post:
          url: "/api/auth/login"
          json:
            email: "test@example.com"
            password: "password123"
          capture:
            json: "$.token"
            as: "token"
      
      - get:
          url: "/api/tasks"
          headers:
            Authorization: "Bearer {{ token }}"
```

### 2. Custom Metrics
Based on our metrics collection:
```javascript
// tests/performance/metrics.js
import { Counter, Trend } from 'k6/metrics'

export const taskLatency = new Trend('task_api_latency')
export const errorRate = new Counter('error_rate')

export function recordMetrics(response, name) {
  taskLatency.add(response.timings.duration, { api: name })
  
  if (response.status >= 400) {
    errorRate.add(1, { api: name })
  }
}
```

## Benchmark Testing

### 1. Benchmark Setup
From our benchmark configuration:
```javascript
// tests/performance/benchmark.js
import { bench, run } from 'mitata'

async function setupBenchmark() {
  const iterations = 1000
  
  bench('API Response Time', async () => {
    const response = await fetch('/api/tasks')
    await response.json()
  })
  
  bench('Database Query Time', async () => {
    await prisma.task.findMany({
      take: 100,
      include: { user: true }
    })
  })
  
  await run({
    iterations,
    warmup: true
  })
}
```

### 2. Memory Profiling
Reference our memory profiling:
```javascript
// tests/performance/memory.js
import v8 from 'v8'

export function measureMemory() {
  const baseHeapSize = v8.getHeapStatistics().used_heap_size
  
  return async (operation) => {
    global.gc?.()
    const beforeHeap = v8.getHeapStatistics().used_heap_size
    
    await operation()
    
    global.gc?.()
    const afterHeap = v8.getHeapStatistics().used_heap_size
    
    return {
      heapGrowth: afterHeap - beforeHeap,
      totalHeapGrowth: afterHeap - baseHeapSize
    }
  }
}
```

## Performance Monitoring

### 1. Metrics Collection
From our metrics setup:
```javascript
// src/monitoring/metrics.js
import prometheus from 'prom-client'

const httpRequestDuration = new prometheus.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code']
})

const activeConnections = new prometheus.Gauge({
  name: 'active_connections',
  help: 'Number of active connections'
})

export const recordMetrics = (req, res, time) => {
  httpRequestDuration.observe(
    {
      method: req.method,
      route: req.route.path,
      status_code: res.statusCode
    },
    time
  )
}
```

### 2. Performance Dashboard
```javascript
// src/monitoring/dashboard.js
import { grafana } from './grafana'

export const performanceDashboard = {
  title: 'API Performance',
  panels: [
    {
      title: 'Request Duration',
      type: 'graph',
      metrics: ['http_request_duration_seconds']
    },
    {
      title: 'Error Rate',
      type: 'graph',
      metrics: ['http_request_errors_total']
    },
    {
      title: 'Active Connections',
      type: 'gauge',
      metrics: ['active_connections']
    }
  ]
}
```

## Performance Analysis

### 1. Reporting
From our reporting setup:
```javascript
// tests/performance/report.js
export function generateReport(results) {
  return {
    summary: {
      totalRequests: results.metrics.iterations.count,
      failedRequests: results.metrics.iterations.failed,
      avgResponseTime: results.metrics.http_req_duration.avg,
      p95ResponseTime: results.metrics.http_req_duration.p95,
      errorRate: results.metrics.http_req_failed.rate
    },
    thresholds: {
      passed: results.thresholds_passed,
      failed: results.thresholds_failed
    },
    trends: {
      responseTime: results.metrics.http_req_duration.trends,
      errorRate: results.metrics.http_req_failed.trends
    }
  }
}
```

### 2. Performance Budgets
```javascript
// tests/performance/budgets.js
export const performanceBudgets = {
  api: {
    responseTime: {
      p95: 500, // 95th percentile should be under 500ms
      median: 200 // median should be under 200ms
    },
    errorRate: {
      max: 0.01 // maximum 1% error rate
    },
    throughput: {
      min: 100 // minimum 100 requests per second
    }
  },
  client: {
    firstPaint: 1000, // under 1 second
    firstContentfulPaint: 1500,
    timeToInteractive: 3000
  }
}
```

## Performance Checklist

### Load Testing
- [ ] Basic load test
- [ ] Stress test
- [ ] Spike test
- [ ] Soak test
- [ ] Breakpoint test

### Metrics
- [ ] Response time
- [ ] Error rate
- [ ] Throughput
- [ ] Resource usage
- [ ] Custom metrics

### Analysis
- [ ] Performance reports
- [ ] Trend analysis
- [ ] Bottleneck identification
- [ ] Optimization recommendations
- [ ] Budget compliance

## Next Steps
1. Set up [Security Testing](../5.%20Security/5c.%20security-testing.md)
2. Configure [API Testing](4b.%20integration-testing.md)
3. Implement [Monitoring](../7.%20Deployment/7d.%20monitoring-setup.md)

## Related Documentation
- [Performance Guide](../6.%20Performance/6a.%20optimization-basics.md)
- [Monitoring Guide](../7.%20Deployment/7d.%20monitoring-setup.md)
- [Testing Guide](4a.%20unit-testing.md)
